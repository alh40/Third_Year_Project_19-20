{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import sys, os\nimport pandas as pd\nimport numpy as np\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D, BatchNormalization\nfrom keras.losses import categorical_crossentropy\nfrom keras.callbacks import ReduceLROnPlateau, TensorBoard, EarlyStopping, ModelCheckpoint\nfrom keras.optimizers import Adam\nfrom sklearn.model_selection import train_test_split\nfrom keras import regularizers\nfrom keras.regularizers import l2\nimport matplotlib.pyplot as plt\n\n#loading previously saved features and labels\na = np.load('./features.npy')\nB = np.load('./labels.npy')\n#taking the mean of the pixel features\na -= np.mean(A, axis = 0)\n#taking the normal standard deviation of the mean of the pixel features\na /= np.std(A, axis = 0)\n\n#Sklearn's test train split function using pseudo random numbers so that everytime the code is run, the output of the split is the same. 80/20 split train/test\nA_train, A_test, B_train, B_test = train_test_split(a, B, test_size = 0.1, random_state = 5)\n#making a seperate split within the train split, to give a validation set\nA_train, A_valid, B_train, B_valid = train_test_split(A_train, B_train, test_size = 0.1, random_state = 5)\n\n#saving test split using a numpy array to be used later\nnp.save('ftestAngry', A_test)\nnp.save('ltestAngry', B_test)\n\n#cnn model design\nmodel = Sequential()\n\nmodel.add(Conv2D(64, kernel_size = (3, 3), activation = 'relu', input_shape = (48, 48, 1), data_format = 'channels_last', kernel_regularizer = l2(0.01)))\nmodel.add(Conv2D(64, kernel_size = (3, 3), activation = 'relu', padding = 'same'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size = (2, 2), strides = (2, 2)))\nmodel.add(Dropout(0.5))\n\nmodel.add(Conv2D(128, kernel_size = (3, 3), activation = 'relu', padding = 'same'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(128, kernel_size = (3, 3), activation = 'relu', padding = 'same'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size = (2, 2), strides = (2, 2)))\nmodel.add(Dropout(0.5))\n\nmodel.add(Conv2D(256, kernel_size = (3, 3), activation = 'relu', padding = 'same'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(256, kernel_size = (3, 3), activation = 'relu', padding = 'same'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size = (2, 2), strides = (2, 2)))\nmodel.add(Dropout(0.5))\n\nmodel.add(Conv2D(512, kernel_size = (3, 3), activation = 'relu', padding = 'same'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(512, kernel_size = (3, 3), activation = 'relu', padding = 'same'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size = (2, 2), strides = (2, 2)))\nmodel.add(Dropout(0.5))\n\nmodel.add(Flatten())\n\nmodel.add(Dense(512, activation = 'relu'))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(512, activation = 'relu'))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(512, activation = 'relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(2, activation = 'sigmoid'))\n\nmodel.summary()\n\n#Compliling the model with adam optimizer and binary crossentropy loss\nmodel.compile(loss = 'binary_crossentropy',\n              optimizer = 'Adam',\n              metrics = ['accuracy'])\n\nmodel_info = model.fit(np.array(A_train), np.array(B_train),\n          batch_size = 64,\n          epochs = 100,\n          verbose = 1,\n          validation_data = (np.array(A_valid), np.array(B_valid)),\n          callbacks = [EarlyStopping(monitor = 'val_loss', patience = int(5))],\n          shuffle = True)\n#saving the  model to be used later for testing\nCNN_json = model.to_json()\nwith open(\"CNNAngry.json\", \"w\") as json_file:\n    json_file.write(CNN_json)\n    model.save_weights(\"CNNAngry.h5\")\n    #adding an element to the end of 'A', then converting the string into a float with a 32-bit single-precision floating-point format\n    A.append(face_features.astype('float32'))\n#puts features into an array\nA = np.asarray(A)\n#adding another dimension\nA = np.expand_dims(A, -1)\n#retrieving labels for training\nB = pd.get_dummies(data['emotion']).as_matrix()\n\n#storing features and labels, respectively, using numpy\nnp.save('features', A)\nnp.save('labels', B)\n\nprint(\"Features: \"+str(len(A[0])))\nprint(\"Dataset Images:\"+str(len(A)))\n\n#plotting results of the training\n\nplt.plot(model_info.history['loss'])\nplt.plot(model_info.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc = 'upper left') \nplt.show()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]}]}