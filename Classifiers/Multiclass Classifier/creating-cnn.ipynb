{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["import sys, os\n","import pandas as pd\n","import numpy as np\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout, Activation, Flatten\n","from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n","from keras.losses import categorical_crossentropy\n","from keras.callbacks import ReduceLROnPlateau, TensorBoard, EarlyStopping, ModelCheckpoint\n","from keras.optimizers import Adam\n","from sklearn.model_selection import train_test_split\n","from keras import regularizers\n","from keras.regularizers import l2\n","import matplotlib.pyplot as plt\n","\n","#loading previously saved features and labels in .npy format\n","a = np.load('./features.npy')\n","B = np.load('./labels.npy')\n","\n","#taking the mean of the pixel features, using the Subtraction Assignment '-='\n","a -= np.mean(A, axis = 0)\n","#taking the normal standard deviation of the mean of the pixel features, using the Division Assignment '/='\n","a /= np.std(A, axis = 0)\n","\n","#Sklearn's test train split function using pseudo random numbers so that everytime the code is run, the output of the split is the same. 80/20 split train/test\n","#random_state = x, where x is the peusdo random number (which can be any positive number) in this case, 5 is used\n","A_train, A_test, B_train, B_test = train_test_split(a, B, test_size = 0.1, random_state = 5)\n","#making a seperate split within the train split, to give a validation set in order to validate during the training of the model\n","A_train, A_valid, B_train, B_valid = train_test_split(A_train, B_train, test_size = 0.1, random_state = 5)\n","\n","#saving test split using a numpy array to be used later \n","np.save('ftest', A_test)\n","np.save('ltest', B_test)\n","\n","#cnn model design\n","model = Sequential()\n","\n","model.add(Conv2D(64, kernel_size = (3, 3), activation = 'relu', input_shape = (48, 48, 1), data_format = 'channels_last', kernel_regularizer = l2(0.01)))\n","model.add(Conv2D(64, kernel_size = (3, 3), activation = 'relu', padding = 'same'))\n","model.add(BatchNormalization())\n","model.add(MaxPooling2D(pool_size = (2, 2), strides = (2, 2)))\n","model.add(Dropout(0.5))\n","\n","model.add(Conv2D(128, kernel_size = (3, 3), activation = 'relu', padding = 'same'))\n","model.add(BatchNormalization())\n","model.add(Conv2D(128, kernel_size = (3, 3), activation = 'relu', padding = 'same'))\n","model.add(BatchNormalization())\n","model.add(MaxPooling2D(pool_size = (2, 2), strides = (2, 2)))\n","model.add(Dropout(0.5))\n","\n","model.add(Conv2D(256, kernel_size = (3, 3), activation = 'relu', padding = 'same'))\n","model.add(BatchNormalization())\n","model.add(Conv2D(256, kernel_size = (3, 3), activation = 'relu', padding = 'same'))\n","model.add(BatchNormalization())\n","model.add(MaxPooling2D(pool_size = (2, 2), strides = (2, 2)))\n","model.add(Dropout(0.5))\n","\n","model.add(Conv2D(512, kernel_size = (3, 3), activation = 'relu', padding = 'same'))\n","model.add(BatchNormalization())\n","model.add(Conv2D(512, kernel_size = (3, 3), activation = 'relu', padding = 'same'))\n","model.add(BatchNormalization())\n","model.add(MaxPooling2D(pool_size = (2, 2), strides = (2, 2)))\n","model.add(Dropout(0.5))\n","\n","model.add(Flatten())\n","\n","\n","model.add(Dense(512, activation = 'relu'))\n","model.add(Dropout(0.4))\n","model.add(Dense(512, activation = 'relu'))\n","model.add(Dropout(0.4))\n","model.add(Dense(512, activation = 'relu'))\n","model.add(Dropout(0.5))\n","\n","model.add(Dense(7, activation = 'softmax'))\n","\n","model.summary()\n","\n","#Compliling the model with adam optimizer and categorical crossentropy loss\n","model.compile(loss = 'categorical_crossentropy',\n","              optimizer = 'Adam',\n","              metrics = ['accuracy'])\n","\n","model_info = model.fit(np.array(A_train), np.array(B_train),\n","#batchsize of 64 seems to be the sweetspot for the Adam optimiser.\n","          batch_size = 64,\n","#An epoch is an iteration over the entire x and y data provided. Model will be run with 100 epochs\n","          epochs = 100,\n","#verbose = 1, shows a progress bar when the model is being run \n","          verbose = 1,\n","#Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data.\n","          validation_data = (np.array(A_valid), np.array(B_valid)),\n","          callbacks = [EarlyStopping(monitor = 'val_loss', patience = int(3))],\n","#Shuffles the order the batches go into the model, so the first batch won't go first for the next epoch for example.\n","          shuffle = True)\n","#saving the  model to be used later for testing\n","CNN_json = model.to_json()\n","with open(\"CNN.json\", \"w\") as json_file:\n","    json_file.write(CNN_json)\n","model.save_weights(\"CNN.h5\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","#plotting results of the training\n","plt.plot(model_info.history['loss'])\n","plt.plot(model_info.history['val_loss'])\n","plt.title('model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'test'], loc = 'upper left') \n","plt.show()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
